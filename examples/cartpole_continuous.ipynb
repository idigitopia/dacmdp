{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f82e61-12f2-4a6e-92e1-cf91c8e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8efecc-4723-4df4-bbf5-b7e6ce882f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n",
      "/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/mujoco_py/builder.py:9: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead\n",
      "  from distutils.sysconfig import customize_compiler\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/glfw/__init__.py:912: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "pybullet build time: Sep 24 2022 15:43:41\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import dacmdp\n",
    "import dacmdp.envs as ce\n",
    "import wandb as wandb_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba47a7-8163-42f3-814e-6b8ca6cc07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import munchify \n",
    "\n",
    "config = munchify({\n",
    "\"envArgs\":{'env_name': 'CartPole-cont-v1', 'seed': 0},\n",
    "\"logArgs\":{'wandb_id': \"cartpole_online_test_1\", \"wandb_entity\":\"dacmdp\",\n",
    "           \"wandb_project\":\"dacmdp_online_test_v0\", \"no_wandb_logging\":True},\n",
    "\"dataArgs\": {'buffer_name': 'random', 'buffer_size': 25000, \n",
    "             'load_buffer': False, 'buffer_device': 'gpu', \"data_dir\":\"\"},\n",
    "\"reprModelArgs\": {'repr_model_name': 'OracleDynamicsRepr', 's_multiplyer': 1, 'a_multiplyer': 10, 'repr_dim': 4},\n",
    "\"actionModelArgs\": {'action_model_name': 'NNActionModelCuda', 'nn_engine': \"torch_pykeops\"},\n",
    "\"mdpBuildArgs\": {'n_tran_types': 10, 'n_tran_targets': 5, 'penalty_beta': 1.0, 'penalty_type': 'linear', 'rebuild_mdpfcache': False,\n",
    "                 'save_mdp2cache': False, 'save_folder': '/nfs/hpc/share/shrestaa/storage/dac_storage_22_Q4/mdp_dumps/random_hash'},\n",
    "\"mdpSolveArgs\": {'device': 'cuda', 'max_n_backups': 5000, \"gamma\": 0.99, 'epsilon': 0.0001, 'penalty_beta': 1, \"operator\": \"simple_backup\"},\n",
    "\"evalArgs\": {'eval_episode_count': 50, \"skip_eval\":True, \"skip_dist_log\":True},\n",
    "})\n",
    "\n",
    "flat_args = lambda config : {f\"{K}::{k}\":v for K in config for k,v in config[K].items() if K != \"flat_args\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcf975-7c0f-43d0-88dd-91ba7602c957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'wandb_id': 'cartpole_online_test_1', 'wandb_entity': 'dacmdp', 'wandb_project': 'dacmdp_online_test_v0', 'no_wandb_logging': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.logArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847177a-2d23-4da5-a65d-6d8d771f0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.logArgs.no_wandb_logging:\n",
    "    wandb_logger.init( id = config.logArgs.wandb_id ,\n",
    "        entity=config.logArgs.wandb_entity,\n",
    "        project=config.logArgs.wandb_project,\n",
    "        config = flat_args(config),\n",
    "        resume = \"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33a45-5551-4247-bf51-d843a8cb5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config.envArgs.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8febdd77-0af2-4583-8a6c-2f6100656d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting buffer!\n",
      "Average Reward of collected trajectories:16.702\n",
      "Collected buffer!\n"
     ]
    }
   ],
   "source": [
    "from dacmdp.core.models_action import NNActionModel, GlobalClusterActionModel, EnsembleActionModel\n",
    "from dacmdp.core.models_sa_repr import OracleDynamicsRepr, DeltaPredictonRepr\n",
    "\n",
    "######### Get Action and Repr Models ####################################\n",
    "seed_buffer = dacmdp.utils_buffer.generate_or_load_buffer(config, env)\n",
    "cluster_action_count = 10\n",
    "cluster_action_model = GlobalClusterActionModel(action_space=env.action_space,\n",
    "                                   n_actions= cluster_action_count,\n",
    "                                   data_buffer=seed_buffer)\n",
    "\n",
    "sa_repr_model = OracleDynamicsRepr(env_name = config.envArgs.env_name)\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de04272-913f-4403-be3e-d49e84d1f87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421352ce-5219-4ee4-a728-8523caf9e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/guille/afern/users/shrestaa/workspaces/dacmdp/dacmdp/envs/cont_cartpole.py:104: UserWarning: \u001b[33mWARN: \n",
      "You are calling 'step()' even though this environment has already returned\n",
      "done = True. You should always call 'reset()' once you receive 'done = True'\n",
      "Any further steps are undnp.float32(low), np.float32(high)efined behavior.\n",
      "                \u001b[0m\n",
      "  logger.warn(\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 10, 1]), torch.Size([100, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_action_model.cand_actions_for_states(torch.FloatTensor(seed_buffer.state[0:100]).cuda()).shape,\\\n",
    "sa_repr_model.encode_state_action_pairs(torch.FloatTensor(seed_buffer.state[0:100]),\n",
    "                                       torch.FloatTensor(seed_buffer.action[0:100])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e057ed19-87e6-4ca0-8533-44bfafab44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacmdp.data.utils_buffer import StandardElasticBuffer\n",
    "global_buffer = StandardElasticBuffer(state_shape = env.observation_space.shape,\n",
    "               action_shape = [len(env.action_space.sample())], # for discrete settings. \n",
    "                batch_size=32, buffer_size=100000, device=\"cpu\")\n",
    "# policy liftup functions\n",
    "def dummy_lifted_policy(Agent, s, epsilon):\n",
    "    if torch.rand(1).item() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        nn_s_idx =  THelper.calc_knn_indices(torch.FloatTensor(s).to(Agent.device), Agent.dacmdp_core.S, 1)[0]\n",
    "        policy_idx = Agent.dacmdp_core.Pi[nn_s_idx] \n",
    "        return Agent.A_names[nn_s_idx,policy_idx].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4057745-881a-4705-930f-c1391ef79243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dacmdp_core_defined\n",
      "Using pre-initialized Action Model GlobalClusterActionModel\n",
      "Using pre-initialized Action Model <dacmdp.core.models_sa_repr.OracleDynamicsRepr object at 0x2b157b4343d0>\n",
      "Average Reward of collected trajectories:18.266\n",
      "Average Reward of collected trajectories:18.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 40it [00:00, 21732.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(10000, 15, 5)\n",
      "nn after consumption,  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 240.43it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 507.83it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 300.83it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 294.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.)\n",
      "500 tensor(0.0066)\n",
      "1000 tensor(9.1553e-05)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 0: Graph built and solved in 1.03 Seconds\n",
      "Average Reward of collected trajectories:170.774\n",
      "Average Reward of collected trajectories:339.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 79it [00:00, 42943.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(20000, 15, 5)\n",
      "nn after consumption,  20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 231.98it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 406.72it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 235.89it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 244.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9289)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 tensor(0.0011)\n",
      "1000 tensor(7.6294e-06)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 1: Graph built and solved in 0.29 Seconds\n",
      "Average Reward of collected trajectories:174.379\n",
      "Average Reward of collected trajectories:436.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 118it [00:00, 64242.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(30000, 15, 5)\n",
      "nn after consumption,  30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 214.00it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 326.85it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 224.91it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 183.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9958)\n",
      "500 tensor(0.0002)\n",
      "1000 tensor(7.6294e-06)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 2: Graph built and solved in 0.36 Seconds\n",
      "Average Reward of collected trajectories:217.87\n",
      "Average Reward of collected trajectories:435.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 157it [00:00, 79023.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(40000, 15, 5)\n",
      "nn after consumption,  40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 190.54it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 316.94it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 201.93it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 193.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9987)\n",
      "500 tensor(0.0006)\n",
      "1000 tensor(0.)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 3: Graph built and solved in 0.38 Seconds\n",
      "Average Reward of collected trajectories:211.56\n",
      "Average Reward of collected trajectories:444.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 196it [00:00, 98088.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(50000, 15, 5)\n",
      "nn after consumption,  50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 166.06it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 271.44it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 178.65it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 166.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9982)\n",
      "500 tensor(0.0002)\n",
      "1000 tensor(0.)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 4: Graph built and solved in 0.46 Seconds\n",
      "Average Reward of collected trajectories:258.667\n",
      "Average Reward of collected trajectories:425.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 235it [00:00, 114345.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(60000, 15, 5)\n",
      "nn after consumption,  60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 152.36it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 244.32it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 171.98it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 155.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9981)\n",
      "500 tensor(0.0020)\n",
      "1000 tensor(2.2888e-05)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 5: Graph built and solved in 0.50 Seconds\n",
      "Average Reward of collected trajectories:224.0\n",
      "Average Reward of collected trajectories:465.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 274it [00:00, 130536.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(70000, 15, 5)\n",
      "nn after consumption,  70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 133.99it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 216.41it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 155.12it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 141.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9978)\n",
      "500 tensor(0.0057)\n",
      "1000 tensor(3.8147e-05)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 6: Graph built and solved in 0.57 Seconds\n",
      "Average Reward of collected trajectories:237.273\n",
      "Average Reward of collected trajectories:419.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 313it [00:00, 141467.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(80000, 15, 5)\n",
      "nn after consumption,  80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 127.58it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 199.86it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 144.76it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 130.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9980)\n",
      "500 tensor(0.0043)\n",
      "1000 tensor(3.0518e-05)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 7: Graph built and solved in 0.61 Seconds\n",
      "Average Reward of collected trajectories:201.72\n",
      "Average Reward of collected trajectories:425.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 352it [00:00, 152994.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(90000, 15, 5)\n",
      "nn after consumption,  90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 116.61it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 184.69it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 136.76it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 119.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9985)\n",
      "500 tensor(0.0001)\n",
      "1000 tensor(7.6294e-06)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 8: Graph built and solved in 0.68 Seconds\n",
      "Average Reward of collected trajectories:266.316\n",
      "Average Reward of collected trajectories:427.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 391it [00:00, 171778.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(100000, 15, 5)\n",
      "nn after consumption,  100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 95.36it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 158.97it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 121.89it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 108.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9978)\n",
      "500 tensor(6.1035e-05)\n",
      "Solved MDP in 500 Backups\n",
      "Epoch 9: Graph built and solved in 0.58 Seconds\n",
      "Average Reward of collected trajectories:221.652\n",
      "Average Reward of collected trajectories:430.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 430it [00:00, 182693.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(110000, 15, 5)\n",
      "nn after consumption,  110000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 102.02it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 157.66it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 121.14it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 104.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9982)\n",
      "500 tensor(0.0007)\n",
      "1000 tensor(0.)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 10: Graph built and solved in 0.79 Seconds\n",
      "Average Reward of collected trajectories:196.296\n",
      "Average Reward of collected trajectories:457.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 469it [00:00, 191466.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(120000, 15, 5)\n",
      "nn after consumption,  120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 92.31it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 144.70it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 110.96it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 94.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9978)\n",
      "500 tensor(0.0009)\n",
      "1000 tensor(7.6294e-06)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 11: Graph built and solved in 0.86 Seconds\n",
      "Average Reward of collected trajectories:266.1\n",
      "Average Reward of collected trajectories:496.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 508it [00:00, 202327.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(130000, 15, 5)\n",
      "nn after consumption,  130000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 87.53it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 136.77it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 106.31it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 89.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9983)\n",
      "500 tensor(7.6294e-05)\n",
      "Solved MDP in 500 Backups\n",
      "Epoch 12: Graph built and solved in 0.67 Seconds\n",
      "Average Reward of collected trajectories:231.87\n",
      "Average Reward of collected trajectories:451.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 547it [00:00, 213580.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_next_states.shape torch.Size([10000, 4])\n",
      "replace indices :  False torch.Size([10000, 4])\n",
      "Instantiated DACMDP for transition Batch\n",
      "(140000, 15, 5)\n",
      "nn after consumption,  140000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate Candidate Actions: : 10it [00:00, 83.12it/s]\n",
      "Calculate/Update Datsaet SA Representation: : 10it [00:00, 129.83it/s]\n",
      "Calculate/Update Candidate Transition SA Representation: : 10it [00:00, 102.67it/s]\n",
      "Update Transition model of core dacmdp: : 10it [00:00, 85.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(99.9980)\n",
      "500 tensor(0.0001)\n",
      "1000 tensor(0.)\n",
      "Solved MDP in 1000 Backups\n",
      "Epoch 13: Graph built and solved in 0.95 Seconds\n",
      "Average Reward of collected trajectories:231.5\n",
      "Average Reward of collected trajectories:469.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caculating State Representations: : 586it [00:00, 223279.63it/s]\n",
      "/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/astroid/node_classes.py:94: DeprecationWarning: The 'astroid.node_classes' module is deprecated and will be replaced by 'astroid.nodes' in astroid 3.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method AutoreloadMagics.post_execute_hook of <IPython.extensions.autoreload.AutoreloadMagics object at 0x2b150115c4c0>> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/IPython/extensions/autoreload.py:615\u001b[0m, in \u001b[0;36mAutoreloadMagics.post_execute_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m newly_loaded_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_modules\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modname \u001b[38;5;129;01min\u001b[39;00m newly_loaded_modules:\n\u001b[0;32m--> 615\u001b[0m     _, pymtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_and_mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pymtime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reloader\u001b[38;5;241m.\u001b[39mmodules_mtimes[modname] \u001b[38;5;241m=\u001b[39m pymtime\n",
      "File \u001b[0;32m/nfs/guille/afern/users/shrestaa/installation_files/MinicondaInstallation/envs/mondac_env4/lib/python3.10/site-packages/IPython/extensions/autoreload.py:212\u001b[0m, in \u001b[0;36mModuleReloader.filename_and_mtime\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     pymtime \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpy_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from dacmdp.core.utils_misc import plot_distributions_as_rgb_array\n",
    "from dacmdp.eval.utils_eval import evaluate_on_env\n",
    "from dacmdp.data.utils_buffer import StandardBuffer\n",
    "from dacmdp.core.dac_core import DACTransitionBatch\n",
    "from dacmdp.core.dac_build import DACBuildWithActionNames\n",
    "from dacmdp.core.utils_knn import THelper\n",
    "\n",
    "total_training_points = 200000\n",
    "point_batch_size = 10000\n",
    "curr_data_points = 0\n",
    "avg_rewards = 0\n",
    "rewards_log = []\n",
    "dac_plot_log = []\n",
    "images = 0\n",
    "\n",
    "# Instantiate Elastic Agent\n",
    "config.mdpBuildArgs.n_tran_types = cluster_action_count + 5\n",
    "elasticAgent = DACBuildWithActionNames( config = config, \n",
    "                                    action_space = env.action_space, \n",
    "                                    action_model = cluster_action_model, # Update this later.\n",
    "                                    repr_model = sa_repr_model, \n",
    "                                    effective_batch_size= 1000, \n",
    "                                    batch_calc_knn_ret_flat_engine = THelper.batch_calc_knn_ret_flat_pykeops\n",
    "                                    )\n",
    "\n",
    "for epoch_i in range(total_training_points//point_batch_size):\n",
    "    ######### TT 1: Data collect   ###########################################################################\n",
    "    random_policy = lambda s:dummy_lifted_policy(elasticAgent,s, epsilon = 1)\n",
    "    explore_policy = lambda s:dummy_lifted_policy(elasticAgent,s, epsilon = 0.1)\n",
    "    optimal_policy = lambda s:dummy_lifted_policy(elasticAgent,s, epsilon = 0)\n",
    "        \n",
    "    data_buffer = StandardBuffer(state_shape = env.observation_space.shape,\n",
    "                           action_shape = [len(env.action_space.sample())], # for discrete settings. \n",
    "                            batch_size=32, buffer_size=point_batch_size, device=\"cpu\")\n",
    "    data_buffer, info_explore = StandardBuffer.populate_buffer(data_buffer, env, \n",
    "                                                   policy = random_policy if epoch_i ==0 else explore_policy,\n",
    "                                                   episode_count=99999, \n",
    "                                                   frame_count=int(point_batch_size/2))\n",
    "    data_buffer, info_optimal = StandardBuffer.populate_buffer(data_buffer, env, \n",
    "                                                   policy = random_policy if epoch_i ==0 else optimal_policy,\n",
    "                                                   episode_count=99999, \n",
    "                                                   frame_count=int(point_batch_size/2))\n",
    "    global_buffer.append_buffer(data_buffer)\n",
    "    ######################################################################################################\n",
    "    \n",
    "    \n",
    "    ######### TT 2: Update Action Model  ###########################################################################\n",
    "    # Action Model\n",
    "    nn_action_model = NNActionModel(action_space = env.action_space,\n",
    "                                   n_actions = 5,\n",
    "                                   data_buffer = global_buffer,\n",
    "                                   nn_engine= config.actionModelArgs.nn_engine,\n",
    "                                   projection_fxn=lambda s: s, \n",
    "                                   )\n",
    "    action_model = EnsembleActionModel(env.action_space,[nn_action_model, cluster_action_model])\n",
    "    # action_model = cluster_action_model\n",
    "    config.mdpBuildArgs.n_tran_types = action_model.n_actions\n",
    "    elasticAgent.action_model = action_model\n",
    "    \n",
    "    sa_repr_model = DeltaPredictonRepr(s_multiplyer=2, \n",
    "                                   a_multiplyer=1,\n",
    "                                   buffer=global_buffer,\n",
    "                                   nn_engine=\"torch_pykeops\")\n",
    "    elasticAgent.repr_model = sa_repr_model\n",
    "    ######################################################################################################\n",
    "    \n",
    "\n",
    "    ######### TT 3: DACMDP Elastic Build   ###########################################################################\n",
    "    transitions = DACTransitionBatch(torch.FloatTensor(data_buffer.state).clone().detach(),\n",
    "                                    torch.FloatTensor(data_buffer.action).clone().detach(),\n",
    "                                    torch.FloatTensor(data_buffer.next_state).clone().detach(),\n",
    "                                    torch.FloatTensor(data_buffer.reward.reshape(-1)).clone().detach(), \n",
    "                                    torch.LongTensor((1- data_buffer.not_done).reshape(-1)).clone().detach())\n",
    "\n",
    "    st = time.time()\n",
    "    elasticAgent.consume_transitions(transitions, verbose = True, batch_size = 1000)\n",
    "    elasticAgent.dacmdp_core.solve(max_n_backups = config.mdpSolveArgs.max_n_backups, \n",
    "                                   penalty_beta = config.mdpSolveArgs.penalty_beta, \n",
    "                                   epsilon = config.mdpSolveArgs.epsilon, \n",
    "                                   gamma = config.mdpSolveArgs.gamma, \n",
    "                                   operator=\"simple_backup\", \n",
    "                                   bellman_backup_batch_size=500)\n",
    "\n",
    "    print(f\"Epoch {epoch_i}: Graph built and solved in {time.time()-st:.2f} Seconds\")\n",
    "    ######################################################################################################\n",
    "\n",
    "    \n",
    "    ######### TT 4: Eval  ###########################################################################\n",
    "    if not config.evalArgs.skip_eval:\n",
    "        st = time.time()\n",
    "        eval_policy = lambda s:dummy_lifted_policy(elasticAgent,s, epsilon = 0)\n",
    "        config.evalArgs.eval_episode_count = 100\n",
    "        avg_rewards, info = evaluate_on_env(env, eval_policy, eps_count=config.evalArgs.eval_episode_count)\n",
    "        eval_time =  time.time()-st\n",
    "        print(f\"Epoch {epoch_i}: Evaluation compltete\", avg_rewards, \"Time: \",eval_time, \"Seconds\")\n",
    "        rewards_log.append(avg_rewards)\n",
    "        \n",
    "    if not config.evalArgs.skip_dist_log:\n",
    "        image_array= plot_distributions_as_rgb_array(elasticAgent.dacmdp_core.mdp_distributions)\n",
    "        dac_plot_log.append(image_array)\n",
    "        images = wandb_logger.Image(image_array, caption=\"MDP Distributions\")\n",
    "    \n",
    "    if not config.logArgs.no_wandb_logging:   \n",
    "        wandb_logger.log({\"epoch_i\":epoch_i, \"buffer_size\":len(global_buffer), \"Average Reward\":avg_rewards,\n",
    "                         \"explore_data_collection_traj_reward\":np.mean(info_explore[\"all_rewards\"]), \n",
    "                         \"optimal_data_collection_traj_reward\":np.mean(info_optimal[\"all_rewards\"]), \n",
    "                         \"mdp_distribution\":images\n",
    "                         })\n",
    "\n",
    "    ######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852fc45-f35c-4976-8dc5-743f5b0a5cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c11c326-c584-4d26-a64f-abb0871489bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2acfb45dff70>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(rewards_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403dd03a-327a-4e25-a91b-55f74528215b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
